# SPX_VIX
## Investigating the time series relationship between SPX and VIX with Robinhood popularity data

<p>I started this project interested in the SPX, VIX relationship and why was it that VIX was the ‘fear indicator’ of the market. In the EDA phase, I started exploring this relationship by charting the SPX against the VIX. Using python pandas, I created some extra columns in the dataframe for measuring the correlation between the two. These charts led to an interesting question of why the correlation between the two, which is usually strongly negative (i.e. close to -1), would sometimes spike positively, which was prevalent in this year. I hypothesized that this year where the options market saw a heavy increase of ‘retail’ money has influenced the VIX and thus the relationship between the SPX and VIX. I was able to find a good proxy representing a measure of ‘retail’ money by using Robinhood’s number of unique users owning a particular stock. This data was available thanks to robintrack.net, which did the work of scraping the Robinhood API hourly for the number of users owning each stock listed on the platform.</p>
<p>Getting this Robinhood data into something I could use was another challenge, as the data was originally separated by each stock, and contained hourly data, but I needed daily. I wrote a function in python that would create a pandas dataframe from joining each of the stocks but would only keep the top 50 stocks as measured by the number of users at the last data point. This function also only kept the max number of users from each day, which created a daily interval set. The dataset was consolidated into a more concise version by measuring the log returns of number of users in each of those top 50 stocks from day to day and averaging that across the dataframe. Thus, we had an appropriate measure of how unique Robinhood users were buying and selling on their platform and created a good proxy of inflows and outflows of retail money into the market.</p>
<p>Armed with this dataset that included the SPX price, VIX index, and log returns of unique Robinhood users, I went about investigating their relationship in R. I knew the VIX would be more closely correlated with the volatility of the log returns of SPX than the SPX itself, because of the nature of what the VIX is trying to index. So, I fit a GARCH(1,1) model to the ln returns of the SPX, which also applied an ARMA(0,2) model. A second order MA term reduced the autocorrelation effects of the ln returns. Now I could use the garch fit to create a volatility time series by taking the @sigma.t from the garch model. The correlation between this SPX ln return volatility series and VIX was high at .91 and didn’t contain any lagged cross correlation. The time series object containing the SPX volatility, VIX, and Robinhood users was condensed to only contain the dates that occurred after Jan 1, 2020 due to the nature of the hypothesis questioning the market effects of this calendar year.</p>
<p>The next step was to fit a ‘lm’ in R that best predicted the SPX volatility using the VIX.  This model turned out to be a second order polynomial model, but the residuals showed heteroskedasticity and autocorrelation. These residuals were squared to reflect magnitude and not direction, and then modeled using auto arima. This fitted an ARMA(1,1) process which was then used to prewhiten the residuals against the Robinhood user log returns using the TSA ‘prewhite’ function. The best cross correlation for these two time series occurred at lag -1 of the Robinhood user ln returns, meaning this series could be predictive of the next day residual error between SPX volatility and the VIX. I fit a new ‘lm’ that included this lag -1 Robinhood data, and it improved the model, returning an adj. r^2 of .94.</p>
<p>Finally, this improved ‘lm’ was used to improve upon an ARIMA process that fit the VIX. For a comparison, an ARIMA process was also fit using only the SPX volatility as the regressor, and my final model used the predictions from the ‘lm’ as the regressor. The ‘SPX volatility’-only model does fine, but it isn’t much help forecasting as the values are correlated with VIX at lag 0 and are essentially moving in tandem. The model including the Robinhood data is using data from lag -1, so could be used successfully to forecast a day out.  The two ARIMA processes were also backtested and the model with Robinhood data decreased the MAPE from .026 to .018 and the MAE from .61 to .44.</p>
<p>This project has taught me to think differently whenever dates are included in a dataset. There exist fundamental behaviors of variables related to time that can be modeled by some simple algorithms such as ARIMA and GARCH that greatly improve predictions and forecasts. I was very content to see how the volatility of the SPX ln returns fitted by a GARCH(1,1) model fit so closely with the VIX. I knew of the relationship between the two before this project only as VIX being a ‘fear index’ but that is not completely the case. The VIX spikes when the SPX drops, but this is due to the volatility of the log returns of the SPX, and that behavior also exists during sharp upturns in the SPX as well. This behavior comes to light especially in 2020, as the SPX prices have both sharply declined and inclined, but the VIX remained high. Without fitting a GARCH model on the SPX, I would have been comparing apples to oranges.</p>
<p>This project also taught me the importance of residuals in determining fit of a model. I took for granted the assumptions of a model and realized how important non-autocorrelation and homoscedasticity in residuals are. It might be hard to remember how many differences, or log transforms are needed to create a model with white noise residuals, but the model is incorrect if these things aren’t taken care of. I was able to create an improved ARIMA model for forecasting the VIX only because I tried to determine why the residuals between the SPX volatility and VIX ‘lm’ fit were behaving the way they were. I understood that the residuals could reflect both magnitude and direction, but it was important to only consider magnitude when considering the hypothesis at hand. All in all, I’m very happy with the conclusions I was able to draw from this project and think this kind of analysis would be futile without time series modeling.</p>
